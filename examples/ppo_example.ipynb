{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_ppo\n",
    "import matplotlib.pyplot as plt\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flock_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ffa015",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = jax.random.PRNGKey(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fe065",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b099435",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 20\n",
    "n_train = 1_500\n",
    "n_train_env = 32\n",
    "n_test_env = 5\n",
    "n_env_steps = 200\n",
    "n_train_epochs = 2\n",
    "mini_batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216dfd5",
   "metadata": {},
   "source": [
    "## Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = flock_env.SimpleFlockEnv(\n",
    "    reward_func=flock_env.rewards.exponential_rewards, n_agents=n_agents\n",
    ")\n",
    "env_params = env.default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2209a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = n_train * n_train_env * n_env_steps * n_train_epochs // mini_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e332f99",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c52ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = jax_ppo.default_params._replace(\n",
    "    gamma=0.99, \n",
    "    gae_lambda=0.99, \n",
    "    entropy_coeff=0.002, \n",
    "    adam_eps=1e-8, \n",
    "    clip_coeff=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schedule = optax.linear_schedule(2e-3, 2e-5, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266aff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, agent = jax_ppo.init_agent(\n",
    "    k,\n",
    "    params,\n",
    "    env.action_space(env_params).shape,\n",
    "    env.observation_space(env_params).shape,\n",
    "    train_schedule,\n",
    "    layer_width=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bfc9c2",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, trained_agent, losses, state_ts, rewards, _ = jax_ppo.train(\n",
    "    k,\n",
    "    env,\n",
    "    env_params,\n",
    "    agent,\n",
    "    n_train,\n",
    "    n_train_env,\n",
    "    n_train_epochs,\n",
    "    mini_batch_size,\n",
    "    n_test_env,\n",
    "    params,\n",
    "    greedy_test_policy=True,\n",
    "    n_env_steps=n_env_steps,\n",
    "    n_agents=n_agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ba6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jnp.mean(jnp.sum(rewards, axis=(2, 3)), axis=1));\n",
    "plt.xlabel(\"Train Step\")\n",
    "plt.ylabel(\"Avg Total Rewards\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "matplotlib.rc('animation', html='html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = flock_env.visualisation.animate_agents(\n",
    "    state_ts.agent_positions[-1, 0],\n",
    "    state_ts.agent_headings[-1, 0],\n",
    "    rewards[-1, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89a93c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539c8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
