from typing import Tuple, Union

import chex
import distrax
import esquilax
import jax.numpy as jnp
import jax.random
import jax_ppo
import optax
from esquilax.ml.rl import AgentState, Trajectory
from flax import linen as nn
from jax_ppo.mlp import algos, policy
from jax_ppo.training import train_step_with_refresh


class PPOAgent(esquilax.ml.rl.Agent):
    def __init__(
        self,
        ppo_params: jax_ppo.PPOParams,
        update_epochs: int,
        mini_batch_size: int,
        max_mini_batches: int,
    ):
        self.params = ppo_params
        self.update_epochs = update_epochs
        self.mini_batch_size = mini_batch_size
        self.max_mini_batches = max_mini_batches

    def init_state(
        self,
        key: chex.PRNGKey,
        observation_shape: Tuple[int, ...],
        n_actions: int,
        schedule: Union[float, optax.Schedule],
        layer_width: int = 64,
        n_layers: int = 2,
        activation: nn.activation = nn.tanh,
    ):
        model = policy.ActorCritic(
            layer_width=layer_width,
            n_layers=n_layers,
            n_actions=n_actions,
            activation=activation,
        )

        tx = optax.chain(
            optax.clip_by_global_norm(self.params.max_grad_norm),
            optax.inject_hyperparams(optax.adam)(
                learning_rate=schedule, eps=self.params.adam_eps
            ),
        )

        return AgentState.init_from_model(key, model, tx, observation_shape)

    def sample_actions(
        self,
        key: chex.PRNGKey,
        agent_state: AgentState,
        observations: chex.Array,
        greedy: bool = False,
    ) -> Tuple[chex.ArrayTree, chex.ArrayTree]:

        if greedy:
            actions, log_std, value = agent_state.apply(observations)
            dist = distrax.MultivariateNormalDiag(actions, jnp.exp(log_std))
            log_likelihood = dist.log_prob(actions)
        else:
            keys = jax.random.split(key, observations.shape[0])
            _, actions, log_likelihood, value = jax.vmap(
                lambda k, obs: algos.sample_actions(k, agent_state, obs), in_axes=(0, 0)
            )(keys, observations)

        return actions, dict(log_likelihood=log_likelihood, value=value)

    def update(
        self,
        key: chex.PRNGKey,
        agent_state: AgentState,
        trajectories: Trajectory,
    ) -> Tuple[AgentState, chex.ArrayTree]:

        # Unpack trajectories generated by Esquilax
        trajectories = jax_ppo.Trajectory(
            state=trajectories.obs,
            action=trajectories.actions,
            log_likelihood=trajectories.action_values["log_likelihood"],
            value=trajectories.action_values["value"],
            reward=trajectories.rewards,
            done=trajectories.done,
        )

        # Reshape trajectories into individual agent histories
        def reshape(x):
            x = jnp.swapaxes(x, 1, 2)
            x = jnp.reshape(x, (x.shape[0] * x.shape[1],) + x.shape[2:])
            return x

        trajectories = jax.tree.map(reshape, trajectories)

        _, agent_state, losses = train_step_with_refresh(
            jax_ppo.mlp.algos.prepare_batch,
            key,
            self.update_epochs,
            self.mini_batch_size,
            self.max_mini_batches,
            self.params,
            trajectories,
            agent_state,
        )
        losses = jax.tree_util.tree_map(jnp.ravel, losses)

        agent_state = AgentState(
            step=agent_state.step,
            apply_fn=agent_state.apply_fn,
            params=agent_state.params,
            tx=agent_state.tx,
            opt_state=agent_state.opt_state,
        )

        return agent_state, losses
