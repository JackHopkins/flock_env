{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage and Plotting of the Multi-Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flock_env import DiscreteActionFlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "matplotlib.rc('animation', html='html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import Agent\n",
    "from collections import deque\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables\n",
    "\n",
    "- `N_AGENTS`: Number of boid agents to include in the flock environment\n",
    "- `RECORD_STEP`: Number of episodes before producing a new plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AGENTS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_STEP = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Training Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is initialized with:\n",
    "\n",
    "- `n_agents`: The number of agents included in the flock\n",
    "- `speed`: Distance agents move each step (the size of environment is 1.0x1.0)\n",
    "- `n_steps`: Number of simulation steps\n",
    "- `rotation_size`: Size of one unit of rotation, given as a fraction of Π (i.e. 0.1 is a rotation size of 0.1Π)\n",
    "- `n_actions`: Number of rotation actions, e.g. value 3 will mean the action space will then be `[-0.1Π, 0, 0.1Π]`\n",
    "- `proximity_threshold`: Distance under which boids are penalised for being too close\n",
    "- `obstacles`: List/tuple of triples describing circular enviromental obstacles. Each triple should be in the format `(x, y, radius)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteActionFlock(n_agents=N_AGENTS,\n",
    "                          speed=0.0125,\n",
    "                          n_steps=10000,\n",
    "                          rotation_size=0.0225,\n",
    "                          n_actions=5,\n",
    "                          distant_threshold=0.05,\n",
    "                          proximity_threshold=0.008, \n",
    "                          obstacles=[(0.5, 0.5, 0.2), (0.1, 0.1, 0.1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN agent is initialized with parameters taken from the environment as well as parameters controlling the DQN itself. Here the `state_size` and `action_size` parameters are the size of the observation and action spaces for a single agent, although the environment requires an array of values for all the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=env.observation_space.shape[0],\n",
    "              action_size=env.action_space.n,\n",
    "              n_agents=env.n_agents, \n",
    "              buffer_size=int(1e5),\n",
    "              batch_size=256,\n",
    "              gamma=0.99,\n",
    "              tau=1e-3,\n",
    "              learning_rate=5e-4,\n",
    "              update_every=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function resets and runs the enviroment with actions from the network. The result is returned as an animated matplotlib quiver plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_plot(steps=2000, eps=0):\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Record positions, headings and rewards\n",
    "    pos = []\n",
    "    rot = []\n",
    "    rwd = []\n",
    "    \n",
    "    # Run the model taking actions from the RL agent\n",
    "    for _ in range(steps):\n",
    "        state, reward, _, _ = env.step(agent.act(state, eps))\n",
    "        pos.append(env.x[:, :env.n_agents].copy())\n",
    "        rot.append(env.theta.copy())\n",
    "        rwd.append(reward)\n",
    "    \n",
    "    pos = np.stack(pos)\n",
    "    rot = np.stack(rot)\n",
    "    rwd = np.stack(rwd)\n",
    "    # Scale rewards to use as colours for the plot\n",
    "    rwd = 255*(rwd-rwd.min())/(rwd.max()-rwd.min())  \n",
    "    \n",
    "    d = np.append(pos, rot[:, np.newaxis, :], axis=1)\n",
    "    d = np.append(d, rwd[:, np.newaxis, :], axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(8, 8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    obstacles = env.x[:, -env.n_obstacles:].T\n",
    "    radii = env.obstacle_radii[0]\n",
    "    for o in zip(obstacles, radii):\n",
    "        draw_circle = plt.Circle((o[0][0], o[0][1]), \n",
    "                                 o[1],\n",
    "                                 fill=True, \n",
    "                                 alpha=0.2, \n",
    "                                 color='r')\n",
    "        ax.add_artist(draw_circle)\n",
    "    \n",
    "    q = ax.quiver(d[0][0], d[0][1], \n",
    "                  np.cos(d[0][2]), \n",
    "                  np.sin(d[0][2]), d[0][3], \n",
    "                  cmap=plt.get_cmap('winter'))\n",
    "    \n",
    "    def update_quiver(f):\n",
    "        \"\"\"Updates the values of the quiver plot\"\"\"\n",
    "        q.set_offsets(f[:2].T)\n",
    "        q.set_UVC(np.cos(f[2]), np.sin(f[2]), f[3])\n",
    "        return q,\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, \n",
    "                                   update_quiver, \n",
    "                                   frames=d[1:],\n",
    "                                   interval=50, \n",
    "                                   blit=False)\n",
    "    \n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=200, max_t=10000, eps_start=1.0, eps_end=0.01, eps_decay=0.95):\n",
    "    scores = []\n",
    "    plots = []\n",
    "    \n",
    "    eps = eps_start \n",
    "    \n",
    "    for i_episode in range(n_episodes+1):\n",
    "        states = env.reset()\n",
    "        inner_scores = list()\n",
    "        score = 0\n",
    "    \n",
    "        for t in range(max_t):\n",
    "            # Call the agent with the local observations for each agent\n",
    "            # then actions is a 2d array of actions for each agent\n",
    "            actions = agent.act(states, eps)\n",
    "            \n",
    "            next_states, rewards, done, _ = env.step(actions)\n",
    "            agent.step(states, actions, rewards, next_states, done)\n",
    "            states = next_states\n",
    "            score += np.mean(rewards)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            inner_scores.append(score)\n",
    "            eps = max(eps * eps_decay, eps_end)\n",
    "\n",
    "        scores.append(inner_scores)\n",
    "        \n",
    "        # Generate a new animated plot after a fixed number of steps\n",
    "        if i_episode%RECORD_STEP==0:\n",
    "            plots.append(test_plot())\n",
    "\n",
    "    return np.array(scores), plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are just for this example, should be a lot larger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores, plots = dqn(max_t=100, n_episodes=10, eps_decay=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(plots):\n",
    "    p.save(f\"videos/{22:03}_{RECORD_STEP*i+1:03}.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
